This program implements the naive bayes and TAN algorithm. It will be written in java for now.
There will be a command line parser where the format of the command is

java bayes <train-set-file> <test-set-file> <n|t>

train-set-file is the training set file.
test-set-file is the testing set file.
Both have an ARFF file format.
This basically means it's in this format:
The file begins with lines starting with % announcing the data's source.
These lines our scanner should ignore.
After %s, there are @relation.
Ignore that as well.
Finally there are @attributes which are random variables and the values they can realize to.
The last @attributes will be named class and that is the variable we are testing for.
If the file doesn't match these formats, we should throw an InvalidFileFormatException.

The last parameter is either n or t telling us whether to use naive bayes rule or TAN.
We should do error checking on the command.

Naive Bayes:
We will create a naive bayes calculation object. This object will create a probability distribution hash
table (hashtable of hashtables). We will need a method to increment a cell given an attribute, 
and calculate probilities for each cell. Each cell is a nX2 hash table with {realization value: probability}.

We will also need a constructor that given a file and a scanner will
create the naive bayes calculation object - this should handle overfitting.

Next is the naive bayes output object. It is given a file, a scanner, and an outputfilename and should output each
prediction probability.
The first lines of the file should entail the attribute name + the parent of that attribute.
If the parent is null, don't create that line.
Every line after that should have <predicted class> <actual class> <probability>
The final line will be how many examples were actually correct.

Should have a constructor and only three methods:
calculateProbability, getLine, writeLine.

TAN:
    This is far harder than naive bayes. 
    We will create a TAN calculation object.
    It will need to have a set of states and run Kruskal's or Prim's
    MST algorithm.
    That will have its own calculation object. The object's specs are:
    Given a set of states and  edges, it will return the MST for that graph.
    We first need to compute and verify the
    information gains for each edge. Once that is verified, we run Prim's algorithm.
    After that, we just add our class node and the traditional naive bayes edges.
    This means the output object calculation will be different...
    I need to figure out the difference though.

Debugging:
Each object will have a toString method. We will use that as our debugging tool.
That is best as we have sample answers and sample graphs - easier verification than
unit tests.
